{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb152d6-21e5-46c6-931d-11e99e6a6798",
   "metadata": {},
   "source": [
    "## Load In Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d560976-afaf-4a08-9913-c679edf0f3e9",
   "metadata": {},
   "source": [
    "To run this demonstration notebook, you will need to have the following packages imported below installed. This may take some time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546caac-b5a5-494f-95e8-19a01e117492",
   "metadata": {},
   "source": [
    "#### Note: Environment setup\n",
    "Running this notebook requires a Microsoft Planatetary Computer API key.\n",
    "\n",
    "To use your API key locally, set the environment variable <i><b>PC_SDK_SUBSCRIPTION_KEY</i></b> or use <i><b>pc.settings.set_subscription_key(<YOUR API Key>)</i></b><br>\n",
    "See <a href=\"https://planetarycomputer.microsoft.com/docs/concepts/sas/#when-an-account-is-needed\">when an account is needed for more </a>, and <a href=\"https://planetarycomputer.microsoft.com/account/request\">request</a> an account if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a03723e-78ae-4150-ba22-e2e485b95cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pandas for working with data in DataFrame structures\n",
    "import pandas as pd\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac5e929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude and Longitude</th>\n",
       "      <th>Class of Land</th>\n",
       "      <th>vh</th>\n",
       "      <th>vv</th>\n",
       "      <th>RVI</th>\n",
       "      <th>RVI(min)</th>\n",
       "      <th>RVI(max)</th>\n",
       "      <th>RVI(median)</th>\n",
       "      <th>vh(min)</th>\n",
       "      <th>vh(max)</th>\n",
       "      <th>vh(median)</th>\n",
       "      <th>vv(min)</th>\n",
       "      <th>vv(max)</th>\n",
       "      <th>vv(median)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10.323727047081501, 105.2516346045924)</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.043734</td>\n",
       "      <td>0.205613</td>\n",
       "      <td>0.640923</td>\n",
       "      <td>0.587695</td>\n",
       "      <td>0.694151</td>\n",
       "      <td>0.640923</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>3.314999</td>\n",
       "      <td>0.038026</td>\n",
       "      <td>0.015913</td>\n",
       "      <td>162.339966</td>\n",
       "      <td>0.148608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10.322364360592521, 105.27843410554115)</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.040909</td>\n",
       "      <td>0.168547</td>\n",
       "      <td>0.701644</td>\n",
       "      <td>0.665469</td>\n",
       "      <td>0.737819</td>\n",
       "      <td>0.701644</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>1.885325</td>\n",
       "      <td>0.033830</td>\n",
       "      <td>0.007239</td>\n",
       "      <td>5.564671</td>\n",
       "      <td>0.143516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10.321455902933202, 105.25254306225168)</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.041375</td>\n",
       "      <td>0.203101</td>\n",
       "      <td>0.619992</td>\n",
       "      <td>0.577957</td>\n",
       "      <td>0.662027</td>\n",
       "      <td>0.619992</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>3.314999</td>\n",
       "      <td>0.035738</td>\n",
       "      <td>0.015913</td>\n",
       "      <td>162.339966</td>\n",
       "      <td>0.145177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10.324181275911162, 105.25118037576274)</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.044391</td>\n",
       "      <td>0.206562</td>\n",
       "      <td>0.645607</td>\n",
       "      <td>0.593127</td>\n",
       "      <td>0.698088</td>\n",
       "      <td>0.645607</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>3.314999</td>\n",
       "      <td>0.038637</td>\n",
       "      <td>0.016310</td>\n",
       "      <td>162.339966</td>\n",
       "      <td>0.149594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10.324635504740822, 105.27389181724476)</td>\n",
       "      <td>Rice</td>\n",
       "      <td>0.042551</td>\n",
       "      <td>0.174365</td>\n",
       "      <td>0.704493</td>\n",
       "      <td>0.655041</td>\n",
       "      <td>0.753944</td>\n",
       "      <td>0.704493</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.467440</td>\n",
       "      <td>0.036355</td>\n",
       "      <td>0.012190</td>\n",
       "      <td>14.588789</td>\n",
       "      <td>0.147110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Latitude and Longitude Class of Land        vh        vv  \\\n",
       "0   (10.323727047081501, 105.2516346045924)          Rice  0.043734  0.205613   \n",
       "1  (10.322364360592521, 105.27843410554115)          Rice  0.040909  0.168547   \n",
       "2  (10.321455902933202, 105.25254306225168)          Rice  0.041375  0.203101   \n",
       "3  (10.324181275911162, 105.25118037576274)          Rice  0.044391  0.206562   \n",
       "4  (10.324635504740822, 105.27389181724476)          Rice  0.042551  0.174365   \n",
       "\n",
       "        RVI  RVI(min)  RVI(max)  RVI(median)   vh(min)   vh(max)  vh(median)  \\\n",
       "0  0.640923  0.587695  0.694151     0.640923  0.003147  3.314999    0.038026   \n",
       "1  0.701644  0.665469  0.737819     0.701644  0.002306  1.885325    0.033830   \n",
       "2  0.619992  0.577957  0.662027     0.619992  0.002989  3.314999    0.035738   \n",
       "3  0.645607  0.593127  0.698088     0.645607  0.003147  3.314999    0.038637   \n",
       "4  0.704493  0.655041  0.753944     0.704493  0.002744  0.467440    0.036355   \n",
       "\n",
       "    vv(min)     vv(max)  vv(median)  \n",
       "0  0.015913  162.339966    0.148608  \n",
       "1  0.007239    5.564671    0.143516  \n",
       "2  0.015913  162.339966    0.145177  \n",
       "3  0.016310  162.339966    0.149594  \n",
       "4  0.012190   14.588789    0.147110  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_data = pd.read_csv(\"crop_data.csv\")\n",
    "crop_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712088a7",
   "metadata": {},
   "source": [
    "## Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec13ce1",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Now let us select the columns required for our model building exercise. We will consider only VV and VH for our model. It does not make sense to use latitude and longitude as predictor variables as they do not have any impact on presence of rice crop.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f74a2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_data = crop_data[['vh', 'vv', 'RVI', 'RVI(min)', 'RVI(max)', 'RVI(median)', 'vh(min)', 'vh(max)', 'vh(median)', 'vv(min)', 'vv(max)', 'vv(median)', 'Class of Land']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29dea3f",
   "metadata": {},
   "source": [
    "### Train and Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a2dfd9",
   "metadata": {},
   "source": [
    "<p align=\"justify\">We will now split the data into 70% training data and 30% test data. Scikit-learn alias “sklearn” is a robust library for machine learning in Python. The scikit-learn library has a <i><b>model_selection</b></i> module in which there is a splitting function <i><b>train_test_split</b></i>. You can use the same.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7ef204",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = crop_data.drop(columns=['Class of Land']).values\n",
    "y = crop_data ['Class of Land'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10,stratify=y,random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544f5bd",
   "metadata": {},
   "source": [
    "### Feature Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ef4fb",
   "metadata": {},
   "source": [
    "<p align=\"justify\"> Before initiating the model training we may have to execute different data pre-processing steps. Here we are demonstrating the scaling of VV and VH variable by using Standard Scaler.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b38b5",
   "metadata": {},
   "source": [
    "<p align = \"justify\">Feature Scaling is a data preprocessing step for numerical features. Many machine learning algorithms like Gradient descent methods, KNN algorithm, linear and logistic regression, etc. require data scaling to produce good results. Scikit learn provides functions that can be used to apply data scaling. Here we are using Standard Scaler.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53a210",
   "metadata": {},
   "source": [
    "<h4 style=\"color:rgb(195, 52, 235)\"><strong>Tip 4 </strong></h4>\n",
    "<p align=\"justify\">Participants might explore other feature scaling techniques like Min Max Scaler, Max Absolute Scaling, Robust Scaling etc.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8297470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5447ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import GridSearchCV\\n\\nlr_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\\nlr_grid_search = GridSearchCV(LogisticRegression(solver='lbfgs'), lr_param_grid, cv=5)\\nlr_grid_search.fit(X_train, y_train)\\nlr_model = lr_grid_search.best_estimator_\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
    "lr_grid_search = GridSearchCV(LogisticRegression(solver='lbfgs'), lr_param_grid, cv=5)\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "lr_model = lr_grid_search.best_estimator_'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d593710d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\\nrf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)\\nrf_grid_search.fit(X_train, y_train)\\nrf_model = rf_grid_search.best_estimator_\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}\n",
    "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_model = rf_grid_search.best_estimator_'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3388dbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create and train neural nets model\\nnn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\\nnn_model.fit(X_train, y_train)\\n\\n# Create and train Naive Bayes model\\nnb_model = GaussianNB()\\nnb_model.fit(X_train, y_train)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create and train neural nets model\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f17546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"gb_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\\ngb_grid_search = GridSearchCV(GradientBoostingClassifier(), gb_param_grid, cv=5)\\ngb_grid_search.fit(X_train, y_train)\\ngb_model = gb_grid_search.best_estimator_\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''gb_param_grid = {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}\n",
    "gb_grid_search = GridSearchCV(GradientBoostingClassifier(), gb_param_grid, cv=5)\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "gb_model = gb_grid_search.best_estimator_'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c64e02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create and train Decision Trees model\\ndt_model = DecisionTreeClassifier()\\ndt_model.fit(X_train, y_train)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create and train Decision Trees model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80cf594c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"knn_param_grid = {'n_neighbors': [3, 5, 7, 10], 'weights': ['uniform', 'distance'], 'p': [1, 2]}\\nknn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5)\\nknn_grid_search.fit(X_train, y_train)\\nknn_model = knn_grid_search.best_estimator_\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''knn_param_grid = {'n_neighbors': [3, 5, 7, 10], 'weights': ['uniform', 'distance'], 'p': [1, 2]}\n",
    "knn_grid_search = GridSearchCV(KNeighborsClassifier(), knn_param_grid, cv=5)\n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "knn_model = knn_grid_search.best_estimator_'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15161bc",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba12fb3",
   "metadata": {},
   "source": [
    "<p justify =\"align\">Now that we have the data in a format appropriate for machine learning, we can begin training a model. In this demonstration notebook, we have used a binary logistic regression model from the scikit-learn library. This library offers a wide range of other models, each with the capacity for extensive parameter tuning and customization capabilities.</p>\n",
    "\n",
    "<p justify =\"align\">Scikit-learn models require separation of predictor variables and the response variable. You have to store the predictor variables in array X and the response variable in the array Y. You must make sure not to include the response variable in array X. It also doesn't make sense to use latitude and longitude as predictor variables in such a confined area, so we drop those too.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecec8444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train logistic regression\n",
    "lr_model = LogisticRegression(solver='lbfgs')\n",
    "lr_model.fit(X_train,y_train)\n",
    "\n",
    "# Create and train random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train neural nets model\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train Decision Trees model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Create and train K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "727cc0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Importing necessary libraries\\nfrom sklearn.model_selection import cross_val_score\\n\\n# Define the models\\nmodels = [lr_model, rf_model, nn_model, nb_model, gb_model, dt_model, knn_model]\\nmodel_names = ['Logistic Regression', 'Random Forest', 'Neural Nets', 'Naive Bayes', 'Gradient Boosting', 'Decision Trees', 'K-Nearest Neighbors']\\n\\n# Perform cross-validation for each model\\nfor model, model_name in zip(models, model_names):\\n    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\\n    print(f'{model_name} - Cross-Validation Accuracy: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})')\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Importing necessary libraries\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the models\n",
    "models = [lr_model, rf_model, nn_model, nb_model, gb_model, dt_model, knn_model]\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Neural Nets', 'Naive Bayes', 'Gradient Boosting', 'Decision Trees', 'K-Nearest Neighbors']\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for model, model_name in zip(models, model_names):\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f'{model_name} - Cross-Validation Accuracy: {scores.mean():.2f} (+/- {scores.std() * 2:.2f})')'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40899e",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9803c",
   "metadata": {},
   "source": [
    "Now that we have trained our model , all that is left is to evaluate it. For evaluation we will generate the classification report and will plot the confusion matrix. Scikit-learn provides many other metrics that can be used for evaluation. You can even write a code on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aefd8d6",
   "metadata": {},
   "source": [
    "### In-Sample Evaluation\n",
    "<p align=\"Jutisfy\"> We will be generating a classification report and a confusion matrix for the training data. It must be stressed that this is in-sample performance testing , which is the performance testing on the training dataset. These metrics are NOT truly indicative of the model's performance. You should wait to test the model performance on the test data before you feel confident about your model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef591ec",
   "metadata": {},
   "source": [
    "In this section, we make predictions on the training set and store them in the <b><i>insample_ predictions</i></b> variable. A confusion matrix is generated to gauge the robustness of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b8f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "insample_predictions_LR = lr_model.predict(X_train)\n",
    "insample_predictions_RF = rf_model.predict(X_train)\n",
    "insample_predictions_NN = nn_model.predict(X_train)\n",
    "insample_predictions_NB = nb_model.predict(X_train)\n",
    "insample_predictions_GB = gb_model.predict(X_train)\n",
    "insample_predictions_DT = dt_model.predict(X_train)\n",
    "insample_predictions_KNN = knn_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1ee358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Logistic Regression******\n",
      "Insample Accuracy 95.56%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       0.96      0.95      0.96       274\n",
      "        Rice       0.95      0.96      0.96       266\n",
      "\n",
      "    accuracy                           0.96       540\n",
      "   macro avg       0.96      0.96      0.96       540\n",
      "weighted avg       0.96      0.96      0.96       540\n",
      "\n",
      "\n",
      "\n",
      "******Random Forest******\n",
      "Insample Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00       270\n",
      "        Rice       1.00      1.00      1.00       270\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "\n",
      "\n",
      "******Neural Nets******\n",
      "Insample Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00       270\n",
      "        Rice       1.00      1.00      1.00       270\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "\n",
      "\n",
      "******Naive Bayes******\n",
      "Insample Accuracy 82.96%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      0.75      0.85       362\n",
      "        Rice       0.66      1.00      0.79       178\n",
      "\n",
      "    accuracy                           0.83       540\n",
      "   macro avg       0.83      0.87      0.82       540\n",
      "weighted avg       0.89      0.83      0.83       540\n",
      "\n",
      "\n",
      "\n",
      "******Gradient Boosting******\n",
      "Insample Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00       270\n",
      "        Rice       1.00      1.00      1.00       270\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "\n",
      "\n",
      "******Decision Tree******\n",
      "Insample Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00       270\n",
      "        Rice       1.00      1.00      1.00       270\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "\n",
      "\n",
      "******KNeighbors******\n",
      "Insample Accuracy 99.63%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      0.99      1.00       272\n",
      "        Rice       0.99      1.00      1.00       268\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('******Logistic Regression******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_LR,y_train)))\n",
    "print(classification_report(insample_predictions_LR,y_train))\n",
    "print('\\n')\n",
    "print('******Random Forest******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_RF,y_train)))\n",
    "print(classification_report(insample_predictions_RF,y_train))\n",
    "print('\\n')\n",
    "print('******Neural Nets******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_NN,y_train)))\n",
    "print(classification_report(insample_predictions_NN,y_train))\n",
    "print('\\n')\n",
    "print('******Naive Bayes******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_NB,y_train)))\n",
    "print(classification_report(insample_predictions_NB,y_train))\n",
    "print('\\n')\n",
    "print('******Gradient Boosting******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_GB,y_train)))\n",
    "print(classification_report(insample_predictions_GB,y_train))\n",
    "print('\\n')\n",
    "print('******Decision Tree******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_DT,y_train)))\n",
    "print(classification_report(insample_predictions_DT,y_train))\n",
    "print('\\n')\n",
    "print('******KNeighbors******')\n",
    "print(\"Insample Accuracy {0:.2f}%\".format(100*accuracy_score(insample_predictions_KNN,y_train)))\n",
    "print(classification_report(insample_predictions_KNN,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8a53b",
   "metadata": {},
   "source": [
    "<p> For plotting a confusion matrix we define the function <b><i>plot_confusion_matrix</i></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6efba0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(models, model_names, X, y, labels):\n",
    "    \"\"\"\n",
    "    Plots confusion matrices for a list of models.\n",
    "\n",
    "    Parameters:\n",
    "    - models: A list of trained models\n",
    "    - model_names: A list of names corresponding to each model\n",
    "    - X: Input features\n",
    "    - y: True labels\n",
    "    - labels: Class labels for the confusion matrices\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "    num_rows = (num_models + 1) // 2\n",
    "    num_cols = 2\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_rows))\n",
    "\n",
    "    for i, (model, model_name) in enumerate(zip(models, model_names), 1):\n",
    "        predictions = model.predict(X)\n",
    "        cm = confusion_matrix(y, predictions)\n",
    "\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f1510f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"models = [lr_model, rf_model, nn_model, nb_model, gb_model, dt_model, knn_model]\\nmodel_names = ['Logistic Regression', 'Random Forest', 'Neural Nets', 'Naive Bayes', 'Gradient Boosting', 'Decision Trees', 'K-Nearest Neighbors']\\ninsample_predictions = [insample_predictions_LR, insample_predictions_RF, insample_predictions_NN,\\n                         insample_predictions_NB, insample_predictions_GB, insample_predictions_DT, insample_predictions_KNN]\\n\\nplot_confusion_matrices(models, model_names, X_train, y_train, labels=['Rice', 'Non Rice'])\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''models = [lr_model, rf_model, nn_model, nb_model, gb_model, dt_model, knn_model]\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Neural Nets', 'Naive Bayes', 'Gradient Boosting', 'Decision Trees', 'K-Nearest Neighbors']\n",
    "insample_predictions = [insample_predictions_LR, insample_predictions_RF, insample_predictions_NN,\n",
    "                         insample_predictions_NB, insample_predictions_GB, insample_predictions_DT, insample_predictions_KNN]\n",
    "\n",
    "plot_confusion_matrices(models, model_names, X_train, y_train, labels=['Rice', 'Non Rice'])'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ab3fc",
   "metadata": {},
   "source": [
    "### Out-Sample Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100e826",
   "metadata": {},
   "source": [
    "When evaluating a machine learning model, it is essential to correctly and fairly evaluate the model's ability to generalize. This is because models have a tendency to overfit the dataset they are trained on. To estimate the out-of-sample performance, we will predict on the test data now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1566bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outsample_predictions_LR = lr_model.predict(X_test)\n",
    "outsample_predictions_RF = rf_model.predict(X_test)\n",
    "outsample_predictions_NN = nn_model.predict(X_test)\n",
    "outsample_predictions_NB = nb_model.predict(X_test)\n",
    "outsample_predictions_GB = gb_model.predict(X_test)\n",
    "outsample_predictions_DT = dt_model.predict(X_test)\n",
    "outsample_predictions_KNN = knn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd2bc9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******Logistic Regression******\n",
      "Accuracy 96.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       0.97      0.97      0.97        30\n",
      "        Rice       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n",
      "\n",
      "\n",
      "*******Random Forest******\n",
      "Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00        30\n",
      "        Rice       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "\n",
      "\n",
      "*******Neural Nets******\n",
      "Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00        30\n",
      "        Rice       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "\n",
      "\n",
      "******Naive Bayes******\n",
      "Accuracy 81.67%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       0.73      1.00      0.85        30\n",
      "        Rice       1.00      0.63      0.78        30\n",
      "\n",
      "    accuracy                           0.82        60\n",
      "   macro avg       0.87      0.82      0.81        60\n",
      "weighted avg       0.87      0.82      0.81        60\n",
      "\n",
      "\n",
      "\n",
      "******Gradient Boosting******\n",
      "Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00        30\n",
      "        Rice       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "\n",
      "\n",
      "******Decision Tree******\n",
      "Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00        30\n",
      "        Rice       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n",
      "\n",
      "\n",
      "******KNeighbors******\n",
      "Accuracy 100.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non Rice       1.00      1.00      1.00        30\n",
      "        Rice       1.00      1.00      1.00        30\n",
      "\n",
      "    accuracy                           1.00        60\n",
      "   macro avg       1.00      1.00      1.00        60\n",
      "weighted avg       1.00      1.00      1.00        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"*******Logistic Regression******\")\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_LR, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_LR))\n",
    "print('\\n')\n",
    "print(\"*******Random Forest******\")\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_RF, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_RF))\n",
    "print('\\n')\n",
    "print(\"*******Neural Nets******\")\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_NN, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_NN))\n",
    "print('\\n')\n",
    "print('******Naive Bayes******')\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_NB, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_NB))\n",
    "print('\\n')\n",
    "print('******Gradient Boosting******')\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_GB, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_GB))\n",
    "print('\\n')\n",
    "print('******Decision Tree******')\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_DT, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_DT))\n",
    "print('\\n')\n",
    "print('******KNeighbors******')\n",
    "print(\"Accuracy {0:.2f}%\".format(100*accuracy_score(outsample_predictions_KNN, y_test)))\n",
    "print(classification_report(y_test, outsample_predictions_KNN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c166037b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"models = [lr_model, rf_model, nn_model, nb_model, gb_model, dt_model, knn_model]\\nmodel_names = ['Logistic Regression', 'Random Forest', 'Neural Nets', 'Naive Bayes', 'Gradient Boosting', 'Decision Trees', 'K-Nearest Neighbors']\\ninsample_predictions = [outsample_predictions_LR, outsample_predictions_RF, outsample_predictions_NN,\\n                         outsample_predictions_NB, outsample_predictions_GB, outsample_predictions_DT, outsample_predictions_KNN]\\n\\nplot_confusion_matrices(models, model_names, X_test, y_test, labels=['Rice', 'Non Rice'])\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''models = [lr_model, rf_model, nn_model, nb_model, gb_model, dt_model, knn_model]\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'Neural Nets', 'Naive Bayes', 'Gradient Boosting', 'Decision Trees', 'K-Nearest Neighbors']\n",
    "insample_predictions = [outsample_predictions_LR, outsample_predictions_RF, outsample_predictions_NN,\n",
    "                         outsample_predictions_NB, outsample_predictions_GB, outsample_predictions_DT, outsample_predictions_KNN]\n",
    "\n",
    "plot_confusion_matrices(models, model_names, X_test, y_test, labels=['Rice', 'Non Rice'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d350e755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latitude and Longitude</th>\n",
       "      <th>Class of Land</th>\n",
       "      <th>vh</th>\n",
       "      <th>vv</th>\n",
       "      <th>RVI</th>\n",
       "      <th>RVI(min)</th>\n",
       "      <th>RVI(max)</th>\n",
       "      <th>RVI(median)</th>\n",
       "      <th>vh(min)</th>\n",
       "      <th>vh(max)</th>\n",
       "      <th>vh(median)</th>\n",
       "      <th>vv(min)</th>\n",
       "      <th>vv(max)</th>\n",
       "      <th>vv(median)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10.18019073690894, 105.32022315786804)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.022501</td>\n",
       "      <td>0.149149</td>\n",
       "      <td>0.488522</td>\n",
       "      <td>0.459407</td>\n",
       "      <td>0.517636</td>\n",
       "      <td>0.488522</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.536283</td>\n",
       "      <td>0.017607</td>\n",
       "      <td>0.003845</td>\n",
       "      <td>1.086427</td>\n",
       "      <td>0.132135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10.561107033461816, 105.12772097986661)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.031486</td>\n",
       "      <td>0.151358</td>\n",
       "      <td>0.628084</td>\n",
       "      <td>0.614971</td>\n",
       "      <td>0.641197</td>\n",
       "      <td>0.628084</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.401623</td>\n",
       "      <td>0.024936</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>1.123161</td>\n",
       "      <td>0.134585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10.623790611954897, 105.13771401411867)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039603</td>\n",
       "      <td>0.136223</td>\n",
       "      <td>0.791227</td>\n",
       "      <td>0.770222</td>\n",
       "      <td>0.812231</td>\n",
       "      <td>0.791227</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.035717</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>3.830926</td>\n",
       "      <td>0.117729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10.583364246115156, 105.23946127195805)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042935</td>\n",
       "      <td>0.147232</td>\n",
       "      <td>0.794516</td>\n",
       "      <td>0.789525</td>\n",
       "      <td>0.799506</td>\n",
       "      <td>0.794516</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>4.416507</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>9.210921</td>\n",
       "      <td>0.053367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10.20744446668854, 105.26844107128906)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027665</td>\n",
       "      <td>0.156761</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.526656</td>\n",
       "      <td>0.581579</td>\n",
       "      <td>0.554118</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>3.002685</td>\n",
       "      <td>0.023604</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>7.247988</td>\n",
       "      <td>0.137934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Latitude and Longitude  Class of Land        vh  \\\n",
       "0   (10.18019073690894, 105.32022315786804)            NaN  0.022501   \n",
       "1  (10.561107033461816, 105.12772097986661)            NaN  0.031486   \n",
       "2  (10.623790611954897, 105.13771401411867)            NaN  0.039603   \n",
       "3  (10.583364246115156, 105.23946127195805)            NaN  0.042935   \n",
       "4   (10.20744446668854, 105.26844107128906)            NaN  0.027665   \n",
       "\n",
       "         vv       RVI  RVI(min)  RVI(max)  RVI(median)   vh(min)   vh(max)  \\\n",
       "0  0.149149  0.488522  0.459407  0.517636     0.488522  0.001122  0.536283   \n",
       "1  0.151358  0.628084  0.614971  0.641197     0.628084  0.000900  0.401623   \n",
       "2  0.136223  0.791227  0.770222  0.812231     0.791227  0.003468  0.313000   \n",
       "3  0.147232  0.794516  0.789525  0.799506     0.794516  0.000641  4.416507   \n",
       "4  0.156761  0.554118  0.526656  0.581579     0.554118  0.001192  3.002685   \n",
       "\n",
       "   vh(median)   vv(min)   vv(max)  vv(median)  \n",
       "0    0.017607  0.003845  1.086427    0.132135  \n",
       "1    0.024936  0.003047  1.123161    0.134585  \n",
       "2    0.035717  0.006694  3.830926    0.117729  \n",
       "3    0.014040  0.001529  9.210921    0.053367  \n",
       "4    0.023604  0.004182  7.247988    0.137934  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_vh_vv_rvi_data = pd.read_csv(\"export_data.csv\")\n",
    "submission_vh_vv_rvi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa6e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission_vh_vv_rvi_data = submission_vh_vv_rvi_data[['vh','vv','RVI']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9f93c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.fit(submission_vh_vv_rvi_data[['vh', 'vv', 'RVI', 'RVI(min)', 'RVI(max)', 'RVI(median)', 'vh(min)', 'vh(max)', 'vh(median)', 'vv(min)', 'vv(max)', 'vv(median)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f231739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling \n",
    "#submission_vh_vv_rvi_data = submission_vh_vv_rvi_data.values\n",
    "transformed_submission_data = sc.transform(submission_vh_vv_rvi_data[['vh', 'vv', 'RVI', 'RVI(min)', 'RVI(max)', 'RVI(median)', 'vh(min)', 'vh(max)', 'vh(median)', 'vv(min)', 'vv(max)', 'vv(median)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f352524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions\n",
    "final_predictions = rf_model.predict(transformed_submission_data)\n",
    "final_prediction_series = pd.Series(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d56a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the results into dataframe\n",
    "submission_df = pd.DataFrame({'id':submission_vh_vv_rvi_data['Latitude and Longitude'].values, 'target':final_prediction_series.values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c33617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10.18019073690894, 105.32022315786804)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(10.561107033461816, 105.12772097986661)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10.623790611954897, 105.13771401411867)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(10.583364246115156, 105.23946127195805)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(10.20744446668854, 105.26844107128906)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>(10.308283266873062, 105.50872812216863)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>(10.582910017285496, 105.23991550078767)</td>\n",
       "      <td>Non Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>(10.581547330796518, 105.23991550078767)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>(10.629241357910818, 105.15315779432643)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>(10.574733898351617, 105.10410108072531)</td>\n",
       "      <td>Rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           id    target\n",
       "0     (10.18019073690894, 105.32022315786804)      Rice\n",
       "1    (10.561107033461816, 105.12772097986661)      Rice\n",
       "2    (10.623790611954897, 105.13771401411867)      Rice\n",
       "3    (10.583364246115156, 105.23946127195805)  Non Rice\n",
       "4     (10.20744446668854, 105.26844107128906)      Rice\n",
       "..                                        ...       ...\n",
       "245  (10.308283266873062, 105.50872812216863)  Non Rice\n",
       "246  (10.582910017285496, 105.23991550078767)  Non Rice\n",
       "247  (10.581547330796518, 105.23991550078767)      Rice\n",
       "248  (10.629241357910818, 105.15315779432643)      Rice\n",
       "249  (10.574733898351617, 105.10410108072531)      Rice\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displaying the sample submission dataframe\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "074024ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping the predictions into a csv file.\n",
    "submission_df.to_csv(\"challenge_1_submission_rice_crop_prediction.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329e07bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import geopandas as gpd\\nfrom shapely.geometry import Point\\nimport matplotlib.pyplot as plt\\n\\n# Read your CSV file\\ndf = pd.read_csv(\"crop_data.csv\")\\n\\n# Assuming your DataFrame is named df and the column containing coordinates is \\'Latitude and Longitude\\'\\n# If the coordinates are in a tuple format within strings, you can convert them to Point objects\\ndf[\\'geometry\\'] = df[\\'Latitude and Longitude\\'].apply(lambda x: Point(float(x.replace(\\'(\\', \\'\\').replace(\\')\\', \\'\\').replace(\\' \\', \\'\\').split(\\',\\')[1]), float(x.replace(\\'(\\', \\'\\').replace(\\')\\', \\'\\').replace(\\' \\', \\'\\').split(\\',\\')[0])))\\n\\n# Create a GeoDataFrame\\ngdf = gpd.GeoDataFrame(df, geometry=\\'geometry\\')\\n\\n# Read built-in world dataset\\nworld = gpd.read_file(gpd.datasets.get_path(\\'naturalearth_lowres\\'))\\n\\n# Extract Vietnam from the world dataset\\nvietnam = world[world[\\'name\\'] == \\'Vietnam\\']\\n\\n# Plotting\\nfig, ax = plt.subplots(figsize=(10, 8))\\nvietnam.plot(ax=ax, color=\\'lightgreen\\')  # Plot Vietnam boundaries\\ngdf.plot(ax=ax, color=\\'red\\', marker=\\'o\\', markersize=50)\\nplt.title(\\'Latitude and Longitude Plot on Vietnam Map\\')\\nplt.show()'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read your CSV file\n",
    "df = pd.read_csv(\"crop_data.csv\")\n",
    "\n",
    "# Assuming your DataFrame is named df and the column containing coordinates is 'Latitude and Longitude'\n",
    "# If the coordinates are in a tuple format within strings, you can convert them to Point objects\n",
    "df['geometry'] = df['Latitude and Longitude'].apply(lambda x: Point(float(x.replace('(', '').replace(')', '').replace(' ', '').split(',')[1]), float(x.replace('(', '').replace(')', '').replace(' ', '').split(',')[0])))\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "\n",
    "# Read built-in world dataset\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# Extract Vietnam from the world dataset\n",
    "vietnam = world[world['name'] == 'Vietnam']\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "vietnam.plot(ax=ax, color='lightgreen')  # Plot Vietnam boundaries\n",
    "gdf.plot(ax=ax, color='red', marker='o', markersize=50)\n",
    "plt.title('Latitude and Longitude Plot on Vietnam Map')\n",
    "plt.show()'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
